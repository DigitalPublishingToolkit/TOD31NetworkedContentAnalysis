---
Pr-id: Networked Content Analysis: The Case of Climate Change
P-id: Theory on Demand
A-id: 32
Type: article
Book-type: anthology
Anthology item: article
Item-id: 03
Article-title: conclusion
Article-status: accepted
Author: Sabine Niederer
Rights: CC BY-NC 4.0
...


# 6. Conclusion

In this book, I argue that the analysis of digital media content needs
an approach that takes into account the specificities of how platforms
and engines serve, format, redistribute, and essentially co-produce
content. These specificities are what I refer to as the *technicity of
content*. The foundational work that established the field of content
analysis, developed within communication science, paved the way for the
analysis of (large) bodies of text for features or (recurring) themes,
in order to identify ‘cultural indicators’ or make other inferences
about societal trends and issues.[^08conclusion_1] [^08conclusion_2] While content analysis has
seen a tremendous uptake across scientific disciplines, the application
of these methods to *networked* web content has presented an ongoing
challenge for researchers of various scholarly disciplines. Therefore, I
propose to improve the adaptability and fit of content analysis to
networked content through a range of digital methods and tools that I
show to be conducive to the task. The work of Klaus Krippendorff, a
major proponent and methodological innovator of content analysis as a
field of media research, is a key driver of my own development of what I
name and develop here as *networked content analysis*. As content
analysis has been inclusive of content (in all shapes and forms) *and
its context* since its early beginnings, its methods only need to be
amended to suit the digital era and deal explicitly with the technicity
of networked content.[^08conclusion_3] I propose to utilize controversy mapping and
digital methods to do so, building on these methods’ respective
actor-/issue-centricity and medium-specificity.

In this book, I develop these research techniques through the analysis
of the climate change controversy, an ongoing debate that takes place
across scientific disciplines and into the public realm, across
platforms, sources, and studies, from the first international climate
skeptics conference of 2008 all the way to 2015. When I started this
research, the climate controversy was publicly understood as historic
but hard to historicize, as it was being lived in real-time. It also
experienced an upswing in debate temperature once skeptics began
organizing themselves in these annual conferences, and as several
publications rose in response to debunk their status and unveil
skeptics' entanglement with industry funding, especially tobacco and oil
industries. The case studies in this book end in 2015, when 198
countries signed the ‘Paris Agreement' to cut back on CO2 emissions in a
joint effort to turn the tide of climate change. As we now know, a year
after that milestone, Donald Trump, announced the United States'
withdrawal from the Paris Agreement, which has lead to a surge in
climate discussions and climate activism in social media and in the
streets.

With this book, I do not aim to contribute to climate *science*, which
is well outside of my area of expertise, but instead to offer a
contribution to the study of online content by developing a networked
content analysis of the climate *controversy* as it is specifically
formatted and transformed by platforms and actors. The study accordingly
follows the climate debate in science, as well as on the web (and Google
Web Search), in Wikipedia and in Twitter, and analyzes how content is
networked there, in order to propose adaptive and sensitive research
techniques appropriate to networked content analysis.

These research techniques draw from existing approaches and methods
developed to study controversies and their actors (controversy analysis)
and social and cultural issues with the web (digital methods).
Controversy analysis gives direction to the study of controversy without
the translation of actor language into preset categories listed in a
codebook.[^08conclusion_4] On the contrary, it makes a case for descriptive research
and advises researchers to launch their inquiries ‘in medias res’ and
describe what they see.[^08conclusion_5] There is no single specific protocol,
toolkit or methodological framework for controversy analysis, but there
are ‘commandments’[^08conclusion_6^08conclusion_7], publications, and an educational program at
Sciences Po in Paris that provide many guidelines for the
operationalization of the mapping of controversies. In my case studies,
which analyze the climate debate on the web, Wikipedia and Twitter, this
leads me to describing the group formations (on the web) of climate
actors, following actors across networked and forked articles about
climate change and related topics (Wikipedia), and exploring and
describing climate change co-hashtag networks in Twitter. Digital
methods are developed at the educational program at the University of
Amsterdam in close kinship to controversy mapping and provide concrete
tools and methods for the study of web-based dynamics of social and
cultural issues. Similar to controversy analysis and content analysis,
digital methods put forward non-intrusive methods, views, aspirations,
and affiliations of issues and their actors, collecting data from
websites and social media activity.

The differences that I outline between content analysis as it was
incepted, controversy mapping, and digital methods, all with certain
limitations, are reiterated throughout the chapters of this book.
Krippendorff's robust articulation of content analysis for a prior media
age conceptually acknowledges but strains methodologically and
tool-wise, to grapple with the networked qualities of online content,
where issues, debates, and actors may spread out or recur across
platforms and other carriers. The addition of controversy analysis
offers a research outlook to follow actors and describe the many
viewpoints and stakeholders present in a debate while being
under-attentive to operationalizing this with regard to networked
content by offering mapping methodologies that deal with digital media
content. This is where digital methods come in, which offer tools to
capture and analyze an issue through networked web content that
otherwise is not available in content-driven communication research. My
main contribution here lies in the combination of these approaches that
makes possible the content analysis of networked content.

Networked content analysis as content analysis that is amended to suit
online networked content enables a researcher to jump in the middle of a
controversy, follow actors and describe these actors' viewpoints in
their own words, employing digital methods to capture and analyze the
substance of debates across platforms. My proposition here is similar to
Susan Herring's 2010 study in so far as I also am interested in a
widening of the paradigm of content analysis with methods from adjoining
scientific disciplines. However, while Herring regards content as
contained in media documents, I argue that such a separation between
content and its carrier cannot hold with networked content. Furthermore,
tracing the discipline of content analysis backward, I note that such a
division between content and form or carrier is quite antithetical to
the way that Content Analysis was originally conceived by Krippendorf.
Nevertheless, Krippendorf's formulations are pre-web. Understanding the
technicities of the platforms that serve and co-produce content today
entails studying platforms' characteristics and identifying the queries
or tools that are necessary to demarcate and analyze networked content.
Studying platforms as socio-technical systems is of the utmost
importance, as they are ‘increasingly embedded in our societies’.[^08conclusion_8] In
this book, I develop such a socio-technological perspective on the
controversy surrounding climate change as presented and debated on the
web.

Krippendorff, in his foundational work, stressed that it is one's
definition of what content is and how that is delimited that leads to
specific kinds of analytical results. As we have seen with the analysis
of web content in the various case studies, it is indeed this very
refinement of defining (the materiality of) content that, with the
recognition of the technology as an active material agent and part of
content leads to a specific demarcation of content online. As I argue in
Chapter 2, the definition and demarcation of content have never been so
straightforward in the case of offline materials, and changing
technologies have further complicated these matters. The digitization
also of analog content has changed the nature of materials already,
raising new questions regarding the inclusion of features and formatting
in the analysis. With hyperlinks, content became networked, and thus, it
became harder again to demarcate and to establish where so-called
content ends. Search engines brought about new ways of presenting and
ranking data, and platformization gives further shape to the
far-stretching entanglement of social media with other web content.[^08conclusion_9]
Network Content Analysis aims to be adaptive to the specific
technicities of platform content; therefore, I approach the climate
debate on each platform with platform-specific ways to define and
delineate the corpus to analyze. In my case study of the web, I
demarcate sources, for instance by taking the speakers list from an
international climate skepticism conference and looking up their
respective websites to use for further research, and by taking the top
results for the query' climate change’ in various languages to measure
the resonance of prominent actors. In the Wikipedia chapter, I discuss a
study of climate change-related articles in which the demarcation occurs
by taking those articles that are reciprocally linked from the climate
change article.[^08conclusion_10] In the Twitter study, I demarcate Tweets by a query
that includes tweets containing climate change or global warming.

The inclusion of web content's technicity into the idea of content
itself then leads to analyses that make use of and deal analytically
with, these technical agents. The collection and analysis of web content
that follows the specificities of each platform, and operationalizes the
specific technicities at play, will lead to more precise analysis, one
that is sensitive to the networked nature and dynamical movement of
online content. I realign my work with Krippendorff's inceptive call to
keep the content together with its carrier (or context), and accordingly
propose that in Networked Content Analysis, researchers include not only
the carrier (e.g. the search engine result, the Wikipedia article, the
tweet) but also the technicity thereof (e.g. the ranking of the search
results, the editing history and content robots of the Wikipedia
article, and the hashtags and retweets networking a collection of
tweets) as part of their analytical approaches. This book offers up new
ways forward for content analysis approaches, methods, and techniques
that are suitable for the study of online networked content.
Rehabilitating the inceptive work of Krippendorff, the contemporary
web-literate approach to networked content analysis that I demonstrate
here remains open to all kinds of content and includes contents’
technicity as part of its research method.

## Applications of Networked Content Analysis I: The Web

In the first case study, I approach the climate controversy by assessing
the positions and affiliations of its actors, starting at the time of
the first international skeptics conference organized by the Heartland
Institute in 2008. I analyze the networks of climate debate actors
(including the conference’s keynote speakers) using scientometric
analysis, as well as techniques that I propose as being fruitful for
networked content analysis. Namely, these are hyperlink analysis and
search engine results resonance analysis, which I use to research the
place and status of climate skepticism within both climate science and
the climate debate as it takes place beyond the scientific literature. I
approach the *networkedness* of content through hyperlinks to analyze
networks of association. Subsequently, by using Google Web Search (to
many a dominant entry point to the web) to demarcate top sources for the
query of climate change, the case study zooms in on climate change
actors and their prominence, as identified by the search engine. Here, I
ask how the technical logic of search might be used to measure such
prominence of actors in a specific issue, in this case, looking at the
resonance of climate change scientists (both skeptical and
non-skeptical) within a demarcated set of websites. I zoom in on a
particularly heated moment of the debate in the Dutch context,
immediately following a publication on the scientific consensus
regarding climate change, published by the Royal Academy of Sciences of
the Netherlands.[^08conclusion_11]

Hyperlink analysis shows a distinct profile for the Dutch skeptics, who
strongly associate themselves with the Anglo-American network that
gathers at the Heartland conferences. Meanwhile, non-skeptical
scientists, those ‘climate-concerned', if you will, show a much more
heterogeneous network, with links to science, government, UN, Worldbank,
and mainstream media. Resonance analysis, in this case, shows less
strong differences between skeptical and non-skeptical scientists, with
both sets of actors resonating across sources, and coming in at the top
and bottom of the search results. There are no sources that mention only
the small sample of non-skeptical scientists without also mentioning the
skeptics, but two sources that only give attention to skeptics. Lastly,
through a close reading of the climate skeptics' websites, I find that
their ‘skeptical' delegitimizing campaigns extend to coverage of topics
well outside of the realm of climate science (e.g., the health dangers
of second-hand smoke). Paired together here, traditional scientometrics
and techniques of networked content analysis offer a fine-grained
picture of the status, group formation, and issue commitments of climate
change skeptics (compared to non-skeptical actors). While with
scientometrics alone I have not been able to identify the skeptics as
entirely separate from climate science as an academic field or
inter-discipline, with Networked Content Analysis I have found divergent
networking behavior, as well as the aforementioned *related* issues,
which qualified them more as *professional skeptics* rather than
professional climate experts.

The main challenge that web-based media presents to traditional content
analysis is that web content is networked, for instance, by hyperlinks.
Another way that it is linked and processed is through social media
buttons, which pull the content of websites into various platforms.[^08conclusion_12]
Furthermore, the fact that web content is often accessed through search
engines such as Google Web Search, which rank and suggest content
through undisclosed and ever-evolving algorithms, is just as
problematic.[^08conclusion_13] Asking subsequently what kind of climate change debate
the web puts forward through such search technicity, I would conclude
from this case study that it demonstrates actor alignment in networks of
affinity, association, critique (as the skeptics linking to their main
object of criticism: IPCC), and aspiration (e.g. in the case study,
Dutch skeptics are hyperlinking to their Anglo-American colleagues, who
do not link back). Resonance analysis reveals on one level the sources
present in the top results of a query, but also the mention such sources
make of specific keywords, or, in this case, actors. A close reading of
these actors' websites establishes the image of their professional
skepticism, problematizing, and delegitimizing the apparent
professionalism of their commitment to climate change as an issue.

## Applications of Networked Content Analysis II: Wikipedia

My second case study focuses on the climate debate on Wikipedia, the
most well known go-to online and free reference system on the web.
Characterizing the project as a socio-technical platform for knowledge
production in the encyclopedic format, in this chapter, I discuss the
dependency of the platform, its various user groups, and its content, on
the (underlying) technicity of Wikipedia. In its status as an
encyclopedia, it seems initially counterintuitive to think of Wikipedia
as a space to study controversy. However, due to the way Wikipedia
content is networked, designed, and managed, the platform has emerged to
be recognized as a unique site for controversy mapping; this is because
an online encyclopedic project is ever exposed as being ‘in the making’.
After discussing in detail the technicities and protocols of the
Wikipedia project, I present two studies that each offered a close
reading of a controversy that takes place behind the scenes of Wikipedia
articles. I choose these specific studies and approaches in order to
make a case for a networked content analysis that uses the
(ever-evolving) technicity of this ubiquitous platform of Wikipedia in
the analysis of a particularly contested and major controversial topic.

Subsequently, the networked content analysis of the climate change
debate on Wikipedia by Gerlitz and Stevenson deploys the hyperlinks
between articles on the topic of climate change to demarcate a network
of related articles, which allows for the study of the composition of
its editors (including active bots) as well as editing activity over
time. Here, networked content analysis permits a historical
reconstruction of the debate, and indicated generic Wikipedia editing
trends over time, but also recognizes issue attention cycles, where ‘new
news’ around the controversy or debate has the effect of spiking
Wikipedia activity across specific pages.[^08conclusion_14] Lastly, heat maps may be
used to signal significant moments in Wikipedia's ‘management' of the
issue of global warming, as I discuss extensively in the chapter. Here,
the technicity of the platform formats content in a way that both its
historicity and conditions of production (e.g., the talk pages) become
visible to both users and researchers.

I am attentive here also to the periodization of research on Wikipedia,
and its uptake by researchers as a tool. The first generation of
scholarly Wikipedia research has focused mainly on the platform's
capacities for crowdsourcing knowledge production, as well as on the
reliability of its co-produced content. I argue for more attention to
the machinery that facilitates and formats this knowledge production.
While traditional content analysis reaches its limits to struggle with
the omnipresence of technical agents in the wiki-platform of Wikipedia,
networked content analysis provides means to properly assess Wikipedia's
content, across articles and language versions. It can, of course, also
still be used to compare web-based encyclopedia content to more static
encyclopedia projects. All such potential research queries demand
appropriate research frameworks and tools capable of capturing how
Wikipedia is socio-technically modulated towards reliability and
consensus over time.

## Applications of Networked Content Analysis III: Twitter

In the final case study of Chapter 5, I study the state of the climate
change debate in Twitter, which I commence by assessing the logic of
this platform and how it networks and circulates its content. Here, I
demarcate a set of climate change-related tweets using a tool called
TCAT, and query the set for the resonance of recognized keywords from
various discourses within the climate change debate. I present the
results of this so-called resonance analysis as discourse-specific
*keyword profiles,* which allows for zooming in on the main actors and
the main content circulating within this subset, providing insight in
the different phases of the climate change debate. Importantly, and
counter to practices of pattern recognition, a close reading of the data
proves necessary to filter the collected data further, towards improved
relevance.

The early applications of traditional content analysis discussed in
Chapter 2 stem from the pre-platform era. Thinking back to the warning
issued by McMillan to researchers wanting to use search engines, we can
imagine the hesitation to work with APIs, and the differences between
free APIs (offering limited amounts of data) and real-time full access
to data (as opposed to for instance Twitter's ‘Firehose' API), which
often comes with a price tag. The main methodological contribution of
this chapter is its development of a means to perform resonance
analysis, where the demarcation of content (based on the literature of
input from subject-matter experts) provides a sample in which the
resonance of actors or keywords can be mapped. Similarly, the
demarcation of tweets visualized through hashtag clusters allows for a
descriptive and exploratory analysis of the debate around climate
vulnerability.[^08conclusion_15^08conclusion_16] The hashtag cluster network, I argue, could be
read as a time slice, presenting the status quo of an issue or debate.
In the case of climate change, this time slice does not merely state
‘what’s happening’ but rather serves as a progress report or awareness
system, addressing present challenges of climate change adaptation and
what is at stake. The keyword profiles, on the other hand, enable a
comparative view, which gives insight into how the discourse has shifted
from mitigation to adaptation, confirming the ‘adaptation turn’, which
has been declared in different realms. Furthermore, these methods enable
a close-up study of the actors at the level of these distinct
discourses. In this way, the Twitter study thus also underlines the
persistent mutual interrelation between news media and platforms,
whereby the platforms may produce news or act as a channel of
distribution and amplification of content, sources, and actors, which I
will reflect upon further in this conclusion.

## Five Key Points

In this book, I discuss different research techniques that I propose
together as an integral starting point for a practice of Networked
Content Analysis. Some of these methods pre-exist my use of them for
this purpose, while others are methodologically amended tools and
techniques of digital methods. I would like to rehearse five key points,
which establish the need for such techniques. Firstly, the main goal of
this book is to develop an adaptive toolkit able to deal with the fact
that different web platforms and engines serve content with different
technicities. As each platform or engine has its own technicity and
therefore requires specific methods and analytical tools, I try to stay
true to the strengths of traditional Content Analysis for the humanities
and social research — the non-intrusiveness of the method, the inclusion
of content in all its shapes and forms, and the attention to the context
of content — while further developing techniques that better adapt to
the specificities of networked content.

Secondly, I find it important to emphasize also in my conclusion that
content currently exists in and through the platforms and engines that
produce it, which means a clean separation of content from its carrier
is no longer feasible. It is now impossible or, at least, inadvisable to
regard a Wikipedia article as entirely separate from its publicly
available production process. Who were the authors? Were there bots
involved? What is being presented as related articles? Which sub-topics
(of an entry on Wikipedia) have become their own dedicated articles?
Which were forked as a means of controversy management? Answers to these
questions are likely to be of great interest and utility to those
invested in Content Analysis in a networked era, and to anyone embarking
on the mapping of a contemporary debate. Krippendorff has laid the
groundwork for such analysis, well prior to content analysis having to
deal with online content.

A third point I want to underline is that networked content also *folds
in* traditional media content. Television news is published online,
discussed in websites; news reports and images populate search engine
results, lead to the creation of Wikipedia articles, or are linked to by
tweets and amplified by retweets. This leads to the entanglement of news
(and other mass) media content, more traditional objects of study of
content analysis, and networked content, the object of study in
networked content analysis. The entangled nature of any media or content
relation is where the focus and benefits of networked content analysis
lie. In the Wikipedia study (of chapter 4), I mention how news events
tend to cause heightened editing activity in related articles. In the
Twitter case (of chapter 5), I discuss how Twitter as a micro-blogging
platform could be approached through more conventional news cycle
analyses but also through ‘meme-tracking’.[^08conclusion_17] In the latter mode,
Twitter as a micro-blog could then be seen as highly responsive to or
even parasitical or imploding of conventional news ‘sites’, echoing and
amplifying news snippets by tweeting and retweeting. Further, as Twitter
is often moving information faster than the news, Twitter content, in
some cases, *is* news. Of course, for these reasons, Twitter is a
popular medium for professional journalists. They bind tweets to their
story, and when their work has been published, they may tweet a link to
that article, using it as a channel for the distribution of their own
work. As news and mass media sources strive to make their content
‘platform-ready’, a term by Helmond, the entanglement of news, other
mass media content, and new platforms have entered the next level.[^08conclusion_18]
Networked content analysis proposes to take this entanglement as a given
and demarcate content through the logic of the platform (as developed in
digital methods) and thus follow the actors across sources (as key to
controversy analysis). The rise of digital media does not mean the end
of traditional mass media, but its reconfiguration as part of online
networked content.

Fourthly, and more conceptually, I would like to propose that when
studying the climate change debate through online content, we may regard
the different platforms as different *windows* on the debate. Rather
than asking ‘What does Twitter say about the controversy,’ or critically
asking ‘Who is on Twitter these days, anyway?’ or ‘Who uses hashtags?’
we may productively ask: ‘What *kind of* climate change debate does
Twitter present?’ ‘And how does this compare and relate to the climate
change debate as presented by Wikipedia (for example)?’ In the climate
change case studies in this book, the web presents a climate debate
maintained by *professional skeptics* with distinct networking behavior
and related issues and specific controversy objects. Wikipedia offers a
view on a successfully forked issue, where the debate had been taken out
of the main article, and the skeptical editors stayed true to the debate
itself, migrating along to the new ‘debate-article' established to
address the controversy. Twitter presents a progress report of climate
change adaptation, attentive to the landscapes and animal species
endangered by climate change. In these ways, considering social media
platforms as *windows on an issue* is also productive for creating a
better understanding of the cultures of use of such platforms.

A fifth point worth mentioning is that while Wikipedia offers public
views on its technicity, the other platforms studied in this book do
not. Google Web Search, through its terms of service, does not allow for
the use of its search engine for anything other than search. So
repurposing the engine as a research device (as discussed in detail by
Weltevrede) goes against its rules and regulations.[^08conclusion_19] Twitter has
various APIs; however, on an interface level, Twitter discloses its
mechanisms of ranking and prioritizing content (and neither does Google
or any social media platform). This point was central to a critical
project titled *The People’s Dashboard*, which I developed together with
Esther Weltevrede, Erik Borra, and others in 2015, and find relevant to
mention briefly here.[^08conclusion_20] The People's Dashboard is a social media
platform plugin that visualizes the entanglement of content and users
with the platform and its technicity. The dashboard is intended to be a
critical layer on top of six different social media platforms: YouTube,
Facebook, Twitter, LastFM, LinkedIn, and Instagram in order to discover
and highlight ‘people’s content’ as a layer on top of the interface. The
plugin, which currently works for the interface of Facebook, color-codes
interfaces of social media platforms according to whether it presents
content *of the people*, or *of the platform* (Figure 18). The project
tries to increase understanding of what is actually *social* on social
media nowadays. For researchers, such an understanding stresses the
necessity to regard technicity as omnipresent, and make explicit how it
is dealt with. This idea is recognized by scholars working with
networked content such as Marres and Moats, who, in an STS-tradition,
call for a symmetrical approach to the study of controversies with
social media content, in which there is as much attention to
‘media-technological dynamics’ as there is to ‘issue dynamics’.[^08conclusion_21]
Networked Content Analysis has a slightly different approach, as it
proposes to include technicity by, straightforwardly, taking the
*networkedness of content* into account. In the various case studies, I
describe how platforms network content differently, and — as stressed in
the first point — how this calls for an adaptive approach to the
analysis of networked content, which is amendable to suit the technicity
of a platform. Making technicity explicit in this way is comparative to
the functionality of the People’s Dashboard, as it offers a view on the
entanglement of user content with the platform.

![](imgs/figure18.png)

Figure 18: The People’s Dashboard. This mockup of the People’s Dashboard
was developed during the Digital Methods Winter School of 2015, as a
critical layer on top of the interfaces of dominant social media
platform interfaces, revealing content of the people and of the
platform. A third category is mixed content, indicating that people’s
content has been re-ordered or repurposed (e.g. Facebook News feed or
birthday notifications). The plugin works with Facebook and is available
on Github: http://bit.ly/peoplesdashboard. Digital Methods Initiative,
‘The People's Dashboard'.

I would like to conclude here that networked content analysis remains
true to its roots in content analysis as an unobtrusive method, while
adapting to the web through medium-specific digital methods and taking
on the research outlook of controversy mapping as a means by which
actors may be followed, viewpoints traced, and presumptions left behind,
in order to capture the richness and specificities of actor language. As
such, it combines the adaptability and medium-specificity of digital
methods and the richness of controversy mapping with the rigor of
content analysis. Networked content analysis, as proposed through these
kinds of imbrications, will give renewed significance to modes and
methods of content analysis appropriate in and for the digital era.

## Technicities in Need of Attention?

In this book, I discuss the technicity of web search and interlinked
websites, Wikipedia articles, and tweets. Of course, I have encountered
many *technicities* beyond these that I did not discuss. Furthermore,
there are many other platforms that could be studied in a networked
content analysis of the climate debate. One could analyze climate
activism in a large social media platform like Facebook, or a smaller
image-based platform such as Pinterest, or study websites of climate
change initiatives in a specific geographic region. For each platform,
it is, in any case, crucial to ask questions that take into account the
technicity of networked content: how is content networked in the
platform, and what kind of issue does the platform present?

Perhaps one technicity that remained especially under-discussed and
under-visualized in the maps is geo-location. And it is this question of
place as an important technical aspect of networked content, which
brings me to address the relationship between my proposed approaches and
the directions and implications it has for future research. In the case
studies, I map the major controversy of climate change, not by
visualizing viewpoints on a traditional geographical map, but by tracing
actors and (sub-) issues across online platforms. I discuss how
platforms and engines such as Google Web Search enable researchers to
focus on a national or language-specific content space, for instance in
the study of Dutch climate change skeptics (in Chapter 3) in the
comparison of language versions of a Wikipedia article (as discussed in
Chapter 4), and through mention made of places in Twitter hashtag
clusters (as described in Chapter 5). However, there are other ways in
which content is geo-located (or geo-tagged) on platforms. There are
social media platforms that are based centrally on the utility of
geo-location, such as Foursquare, a service that allows for ‘checking
in' on a specific location and thus sharing where you are with your
followers, or Instagram, which offers the possibility to give your photo
a geo-tag and is for this reason an app often used to share pictures of
hotspots in specific places. Here again, social media can offer a lens
or window to a specific place, and it could be interesting and
productive to ask not only *what kind of place* is this, but also what
kinds of mediations of such places, do specific platforms put forward.

As an example outside of my focus on climate change, in a study of the
city of Amsterdam through social media data, our work at the Digital
Methods initiative recently found that Instagram offers a collected
‘boutique view’ on the city, while meetup.com (a platform for organizing
social gatherings) highlights the ‘tech’ and ‘sports’ venues of the city
of Amsterdam.[^08conclusion_22] For the case study of my book, this means I could
select in the Twitter dataset only the geo-demarcated Tweets, or instead
look at user-profiles and only select those that state a location. This
way, I could research how the state of the debate differs across
geo-locations by looking at the origin of a tweet or of the Twitter user
(profile). On a methodological level, I could assess the possibilities
and limitations of studying place through networked content analysis,
assessing how different platforms deal differently with the demarcation
of place.[^08conclusion_23]

As other technicities add layers to the analysis of an issue or debate,
the diversity of content types included in such a 'layered' networked
content analysis adds complexity to the analysis. Here, we can learn
from controversy mapping, whose scholars have warned against the
creation of an all-encompassing ‘mother map' that includes all actors,
viewpoints, and sources of a certain debate as seen from above.[^08conclusion_24] As
there is no *above* in controversy mapping, these layers should not be
used to create a summary but rather treated as separate mappings, in
which each offers a detailed window through which we can navigate a
debate in all its richness.[^08conclusion_25]

In this book, networked content analysis is developed to study the
climate debate, a controversy that takes place in science and well
beyond, in news media and public debates, and echoing complexly across
online platforms. While this book has put forward several research
techniques, the example of geo-location indicates that further research
will only lead to more material for the content analyst who wants to use
networked content for researching debates and controversy. Furthermore,
it underlines the need for a more thorough understanding of technicity
of content and the adaptive analytical attitude researchers of online
networked content need to develop.

## The Future of Content: Challenges for Further Research

The biggest challenge for researchers who want to work with networked
content may be the multifariousness of content types, data sources, and
technicities, which, in order to be compared need to be somehow
*comparable*. Here, it is useful to consider how both controversy
mapping and digital methods approach this issue. Controversy analysis
does not strive for a clean objective picture to arise from the analysis
of complex issues and debates. Rather than striving for objectivity,
controversy analysis tries to reach what Latour calls ‘second-degree
objectivity’ which is ‘the effort to consider as much subjectivity as
possible. Unlike first-degree objectivity, which defines a situation of
a collective agreement, second-degree objectivity is attained by
revealing the full extent of actors' disagreement and is thereby typical
of controversial settings.’[^08conclusion_26] In second-degree objectivity, it is not
necessary to normalize or objectify content in order to make it
comparable. Instead, it is the wide array of viewpoints, actors, and
sources that build a cartography that Latour himself describes to his
students as ‘observing and describing’.[^08conclusion_27] As controversy mapping does
not offer an operationalization of this approach, let alone how to apply
it to networked content, it is useful here to look at digital methods
for ‘cross-platform analysis’.[^08conclusion_28]

Digital methods have proposed three approaches to cross-platform
analysis, which are strongly related to the methodological difficulties
discussed of disentangling content from online platforms. The first
approach can be summed up as *medium research* and takes as a point of
departure the question of what the platform *does to the content*. How
does the platform rank, obfuscate or amplify specific content, and what
do we know of its cultures of use? A second approach is that of *social
research*. Here, platform technicities are not included in the study, as
the researcher focuses on the story told by the content. A third
approach is the combination of the two, asking both what the platform
does to the content and what stories the content tells.[^08conclusion_29^08conclusion_30] This
approach would be most suitable to Networked content analysis, where we
could explicitly add how the platforms network content, and how content
is ‘inter-linked, inter-liked and inter-hashtagged’.[^08conclusion_31] However,
noting the size of data sets and the necessity of close reading, the
scaling up of such methods remains a challenge, which is dealt with by
various scholarly fields (ranging from humanities to data science).

The comparability of content from different platforms and the web also
becomes an issue in its visualization, or more specifically, in its
side-by-side representation in dashboards. As analysts, activists, and
decision-makers increasingly make use of dashboards, there is increased
urgency to developing critical dashboards, as I alluded to in my
mentioning of the *People’s Dashboard*. A critical dashboard would show
the technicity of content and explain what is left out, what is
foregrounded, and what is being amplified by the logic of the platform.

In the preface to his 2010 manifesto *You Are Not a Gadget*, Jaron
Lanier writes:

> It is early in the twenty-first century, and that means that these
> words will mostly be read by nonpersons – automatons or numb mobs
> composed of people who are no longer acting as individuals. The words
> will be minced into atomized search-engine keywords within industrial
> cloud computing facilities located in remote, often secret locations
> around the world. They will be copied millions of times by algorithms
> designed to send an advertisement to some person somewhere who happens
> to resonate with some fragment of what I say.[^08conclusion_32]

The future of content presented by Lanier, as material increasingly
intertwined with its carriers and platforms, is a future of content
networked to the extreme. We will find content made for the network,
re-hashed, redistributed, and copied by network infrastructure, and then
clicked on, liked, or retweeted by its recipients. The future of content
then is content that is written for exponentially *networked
technicity*. As content will evolve along with the technicity of its
medium, researchers will have to expand our techniques and tools for
networked content analysis, continue to develop a critical vocabulary,
and produce further concepts and visual languages for the mapping,
analysis, and description of networked content.

[^08conclusion_1]: Gerbner, ‘Toward "Cultural Indicators"’.

[^08conclusion_2]: Gerbner, ‘Cultural Indicators’.

[^08conclusion_3]: Krippendorff, *Content Analysis,* 2004.

[^08conclusion_4]: Latour, *Reassembling the Social.*

[^08conclusion_5]: Latour, *Reassembling the Social,* 27.

[^08conclusion_6]: Venturini, *Diving in Magma*.

[^08conclusion_7]: Latour, *Reassembling the Social,* 27.

[^08conclusion_8]: Lazer et al. ‘The Parable of Google Flu’, 1205.

[^08conclusion_9]: Helmond, *The Web as Platform.*

[^08conclusion_10]: ‘Reciprocal linking’ here means both linking to and receiving a
    link from the article on climate change.

[^08conclusion_11]: KNAW, *Klimaatverandering, Wetenschap en Debat.*

[^08conclusion_12]: See also Helmond, *The Web as Platform.*

[^08conclusion_13]: McMillan, ‘The Microscope and the Moving Target’.

[^08conclusion_14]: These general trends include an overall increase of editing
    interventions over time, a relative decrease in activity in the
    months of June and December, and the existence of an incubation
    period between an article's creation and its maturation, where after
    initial editing and a period of inactivity are followed by more
    regular editing.

[^08conclusion_15]: Savage, ‘Contemporary Sociology and the Challenge of Descriptive
    Assemblage'.

[^08conclusion_16]: Tukey, ‘Exploratory Data Analysis’.

[^08conclusion_17]: Leskovec et al. ‘Meme-tracking and the Dynamics of the News
    Cycle’.

[^08conclusion_18]: Helmond, *The Web as Platform.*

[^08conclusion_19]: Weltevrede, *Repurposing Digital Methods.*

[^08conclusion_20]: *The People’s Dashboard* is described extensively on the wiki
    project page, and the team members are listed in the
    ‘Acknowledgements of collaborative work’ section. Digital Methods
    Initiative, ‘The People's Dashboard', 2015,
    https://wiki.digitalmethods.net/Dmi/PeoplesDashboard.

[^08conclusion_21]: Marres and Moats, ‘Mapping Controversies with Social Media’.

[^08conclusion_22]: The layered interactive map is available on:
    http://bit.ly/amsterdamcartodb, the project page is on the Digital
    Methods Initiative wiki. Its project page can be found at:
    https://wiki.digitalmethods.net/Dmi/TheCityAsInterface.

[^08conclusion_23]: A project that explicitly deals with these questions is The
    Knowledge Mile Atlas, in which I am working with information
    designers to create an atlas of a small urban area in Amsterdam.
    Here, we represent different online data sets of a geographic area
    by using different methods of geo-demarcation, data analysis, and
    visualization. First, by geo-locating addresses coming from
    administrative databases, we showed the density of and the
    connections between companies registered in the area. Secondly,
    using natively digital geo-coded objects, such as Foursquare
    check-ins and geotagged photos, we layered the social media view of
    the area. Finally, querying street names in the dominant search
    engine, we collected the online image of each street. Each layer
    offered a methodological exercise in rethinking geo-location based
    on the specificity of each platform and the technicity of its
    content. What is relevant in such methods is the ability to layer
    the online activity on top of the map of the actual geo-location.
    The Knowledge Mile maps show the online presence and resonance of an
    urban area under development in Amsterdam that cuts through the city
    center and crosses many district and neighborhood ‘borders'.
    Niederer et al. ‘Street-Level City Analytics’.

[^08conclusion_24]: Venturini, ‘Diving in Magma’.

[^08conclusion_25]: Venturini, ‘What Is Second-degree Objectivity and How Could It Be
    Represented'.

[^08conclusion_26]: Venturini, ‘Building on Faults', 270.

[^08conclusion_27]: Venturini, ‘Building on Faults', 270.

[^08conclusion_28]: Digital Methods Initiative, *DMIR Unit \#5.*

[^08conclusion_29]: In networked content analysis, this would be explicitly: ‘How
    does the platform *network* content?’

[^08conclusion_30]: Digital Methods Initiative, *DMIR Unit \#5.*

[^08conclusion_31]: Digital Methods Initiative, *DMIR Unit \#5.*

[^08conclusion_32]: J. Lanier, *You Are Not a Gadget: A Manifesto,* New York, NY:
    Alfred A. Knopf, 2010, xiii.
